# ğŸ¯ å®Œæ•´æ”¹è¿›è¯´æ˜

## ä½ æå‡ºçš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### âœ… é—®é¢˜1ï¼šDeletionå®éªŒBug
**é—®é¢˜ï¼š** `ValueError: At least one stride in the given numpy array is negative`

**åŸå› ï¼š** ä½¿ç”¨ `np.argsort()[::-1]` åè½¬æ•°ç»„æ—¶åˆ›å»ºäº†è´Ÿstrideçš„view

**è§£å†³ï¼š**
```python
# åœ¨ deletion_experiment.py çš„ mask_timesteps() ä¸­æ·»åŠ ï¼š
if isinstance(indices, np.ndarray):
    indices = indices.copy()  # å¤åˆ¶æ•°ç»„é¿å…è´Ÿstride
```

**æ–‡ä»¶ï¼š** `deletion_experiment.py` (å·²ä¿®å¤)

---

### âœ… é—®é¢˜2ï¼šå¦‚ä½•ä½¿ç”¨Transformerç­‰å…¶ä»–æ¨¡å‹

**è§£å†³æ–¹æ¡ˆï¼š** åˆ›å»º `enhanced_model_loader.py`

**æ”¯æŒçš„æ¨¡å‹ï¼š**
1. **TSTransformer** - æ—¶é—´åºåˆ—Transformer
2. **DeepResNet** - æ›´æ·±çš„ResNet (ResNet-18/34é£æ ¼)
3. **è‡ªå®šä¹‰æ¨¡å‹æ³¨å†Œæœºåˆ¶**

**ä½¿ç”¨æ–¹æ³•ï¼š**
```python
# æ–¹æ³•1ï¼šç›´æ¥åˆ›å»º
from enhanced_model_loader import TSTransformer
model = TSTransformer(
    input_channels=1,
    num_classes=2,
    d_model=128,
    nhead=8,
    num_layers=3
)

# æ–¹æ³•2ï¼šæ³¨å†Œè‡ªå·±çš„æ¨¡å‹
from enhanced_model_loader import register_custom_model

class MyModel(nn.Module):
    def __init__(self, input_channels, num_classes, length):
        # ä½ çš„æ¨¡å‹å®šä¹‰
        ...

register_custom_model('MyModel', MyModel)
model = create_model('MyModel', input_channels=1, num_classes=2)
```

**æ–‡ä»¶ï¼š** `enhanced_model_loader.py`

---

### âœ… é—®é¢˜3ï¼šè®­ç»ƒä¸è¯„ä¼°åˆ†ç¦»

**è§£å†³æ–¹æ¡ˆï¼š** åˆ›å»º `separate_train_eval.py`

**æ ¸å¿ƒè®¾è®¡ï¼š**
```
è®­ç»ƒé˜¶æ®µï¼ˆç‹¬ç«‹ï¼‰          è¯„ä¼°é˜¶æ®µï¼ˆç‹¬ç«‹ï¼‰
    â†“                          â†“
ModelTrainer              ExplanationEvaluator
    â†“                          â†“
ä¿å­˜æ¨¡å‹                   åŠ è½½æ¨¡å‹
    â†“                          â†“
trained_models/           è¿è¡Œè§£é‡Šåˆ†æ
my_model_best.pth         è¿è¡Œå®éªŒ
                              â†“
                          ä¿å­˜ç»“æœ
```

**ä½¿ç”¨æ–¹æ³•ï¼š**

**è®­ç»ƒé˜¶æ®µï¼š**
```python
from separate_train_eval import ModelTrainer

trainer = ModelTrainer(model, device='cuda', save_dir='./trained_models')

history = trainer.train(
    train_loader,
    test_loader,
    epochs=50,
    model_name='my_model',
    early_stopping_patience=10
)

# æ¨¡å‹è‡ªåŠ¨ä¿å­˜åˆ°: ./trained_models/my_model_best.pth
```

**è¯„ä¼°é˜¶æ®µï¼ˆå¯ä»¥åœ¨ä¸åŒæ—¶é—´/ä¸åŒæœºå™¨è¿è¡Œï¼‰ï¼š**
```python
from separate_train_eval import ExplanationEvaluator

evaluator = ExplanationEvaluator(
    model_path='./trained_models/my_model_best.pth',
    device='cuda'
)

# é‡å»ºæ¨¡å‹æ¶æ„
model = SimpleResNet(...)  # æˆ–ä½ çš„æ¨¡å‹
evaluator.load_model(model)

# è¿è¡Œè§£é‡Šåˆ†æ
results = evaluator.run_explanation_analysis(test_loader, pipeline)

# è¿è¡Œdeletionå®éªŒ
deletion_results = evaluator.run_deletion_experiment(
    test_loader, pipeline, deletion_exp
)
```

**æ–‡ä»¶ï¼š** `separate_train_eval.py`

---

### âœ… é—®é¢˜4ï¼šTime-step-level Consistency

**é—®é¢˜ï¼š** åŸç‰ˆè®¡ç®—çš„æ˜¯å…¨å±€view-levelç›¸ä¼¼åº¦ï¼Œä¸å¤Ÿç²¾ç»†

**æ”¹è¿›ï¼š** åˆ›å»º `enhanced_consistency_trust.py`

**å…³é”®åŒºåˆ«ï¼š**

**åŸç‰ˆï¼ˆGlobal View-levelï¼‰ï¼š**
```python
# è®¡ç®—ä¸¤ä¸ªviewçš„æ•´ä½“ç›¸ä¼¼åº¦
similarity = cosine_similarity(attr1, attr2)
# ç»“æœï¼šå•ä¸ªæ ‡é‡ (å¦‚ 0.85)
```

**æ”¹è¿›ç‰ˆï¼ˆTime-step-levelï¼‰ï¼š**
```python
# å¯¹æ¯ä¸ªæ—¶é—´æ­¥å•ç‹¬è®¡ç®—
for t in range(length):
    values_at_t = [attr1[t], attr2[t], attr3[t], ...]
    consistency[t] = 1.0 / (1.0 + std(values_at_t))

# ç»“æœï¼šæ¯ä¸ªæ—¶é—´æ­¥ä¸€ä¸ªåˆ†æ•° [length]
# ä¾‹å¦‚ï¼š[0.9, 0.7, 0.95, 0.6, ...]
```

**ä¸ºä»€ä¹ˆè¿™æ›´å¥½ï¼š**
- å¯ä»¥çœ‹åˆ°**å“ªäº›æ—¶é—´æ­¥**è·¨è§†å›¾ä¸€è‡´
- ä¸æ˜¯æ‰€æœ‰æ—¶é—´æ­¥éƒ½ä¸€æ ·é‡è¦
- å…³é”®æ—¶é—´æ®µåº”è¯¥æœ‰æ›´é«˜çš„ä¸€è‡´æ€§

**ä½¿ç”¨æ–¹æ³•ï¼š**
```python
from enhanced_consistency_trust import TimestepLevelConsistency

consistency = TimestepLevelConsistency.compute_timestep_consistency(
    attributions={'view1': attr1, 'view2': attr2, ...},
    method='std'  # æˆ– 'range', 'cv'
)

# ç»“æœï¼š[length] æ•°ç»„
print(f"æ—¶é—´æ­¥0çš„ä¸€è‡´æ€§: {consistency[0]}")
print(f"æ—¶é—´æ­¥1çš„ä¸€è‡´æ€§: {consistency[1]}")
```

**æ–‡ä»¶ï¼š** `enhanced_consistency_trust.py`

---

### âœ… é—®é¢˜5ï¼šåŒºåˆ† Attribution Importance vs Explanation Reliability

**æ ¸å¿ƒæ¦‚å¿µï¼š**

**Attribution Importanceï¼ˆå½’å› é‡è¦æ€§ï¼‰ï¼š**
- è¯¥æ—¶é—´æ­¥å¯¹é¢„æµ‹çš„å½±å“ç¨‹åº¦
- **é«˜å€¼** = è¯¥æ—¶é—´æ­¥å¾ˆé‡è¦
- ç”¨ `|attribution|` çš„å¤§å°è¡¡é‡

**Explanation Reliabilityï¼ˆè§£é‡Šå¯é æ€§/Trustï¼‰ï¼š**
- è¯¥è§£é‡Šçš„å¯ä¿¡ç¨‹åº¦
- **é«˜å€¼** = è¯¥è§£é‡Šå¾ˆå¯é ï¼ˆä¸æ˜¯è¯´é‡è¦ï¼‰
- ç”¨ `ä½uncertainty + é«˜consistency` è¡¡é‡

**å››ç§ç»„åˆï¼š**
```
1. é«˜importance + é«˜reliability â†’ çœŸæ­£çš„å…³é”®æ—¶é—´æ­¥ âœ“
2. é«˜importance + ä½reliability â†’ è™šå‡çš„é‡è¦æ€§ âœ— (å±é™©!)
3. ä½importance + é«˜reliability â†’ ç¡®å®ä¸é‡è¦ âœ“
4. ä½importance + ä½reliability â†’ ä¸ç¡®å®š ?
```

**å¦‚ä½•åœ¨æ–‡å­—ä¸ŠåŒºåˆ†ï¼š**

**è®ºæ–‡å†™ä½œå»ºè®®ï¼š**
```
âŒ é”™è¯¯è¡¨è¿°:
"We compute the trust score to identify important timesteps."

âœ“ æ­£ç¡®è¡¨è¿°:
"We compute the trust score to assess the RELIABILITY of attributions, 
not their importance. A high trust score indicates that the attribution 
is STABLE and CONSISTENT across views, regardless of its magnitude."

âŒ é”™è¯¯è¡¨è¿°:
"Trust-weighted attribution gives more weight to important timesteps."

âœ“ æ­£ç¡®è¡¨è¿°:
"Trust-weighted attribution DOWN-WEIGHTS unreliable explanations while 
preserving the original importance ranking of reliable timesteps."
```

**æœ¯è¯­ä½¿ç”¨è§„èŒƒï¼š**
- **Importance / Saliency / Relevance** â†’ ç”¨äºæè¿°attributionå¤§å°
- **Reliability / Trustworthiness / Confidence** â†’ ç”¨äºæè¿°trust
- **Stability / Robustness** â†’ ç”¨äºæè¿°trustçš„æ€§è´¨

**ä½¿ç”¨æ–¹æ³•ï¼š**
```python
from enhanced_consistency_trust import ImportanceVsReliability

# 1. è®¡ç®—çº¯å¯é æ€§ï¼ˆä¸è€ƒè™‘importanceï¼‰
reliability = ImportanceVsReliability.compute_reliability_metrics(
    uncertainty, consistency, alpha=0.5
)

# 2. åˆ†ç±»æ—¶é—´æ­¥
categories = ImportanceVsReliability.categorize_timesteps(
    attribution,  # importance
    reliability,  # trust
    importance_threshold=0.5,
    trust_threshold=0.6
)

# 3. åˆ†æç»“æœ
print(f"å¯ä¿¡çš„é‡è¦ç‚¹: {len(categories['reliable_important'])}")
print(f"ä¸å¯ä¿¡çš„é‡è¦ç‚¹: {len(categories['unreliable_important'])}")
```

**æ–‡ä»¶ï¼š** `enhanced_consistency_trust.py`

---

### âœ… é—®é¢˜6ï¼šéªŒè¯ Trust â‰  Importance çš„å®éªŒè®¾è®¡

**æ ¸å¿ƒæ€æƒ³ï¼š** 
å¦‚æœtrustçœŸçš„åæ˜ å¯é æ€§è€Œä¸æ˜¯é‡è¦æ€§ï¼Œé‚£ä¹ˆï¼š
- **é«˜trustçš„æ—¶é—´æ­¥åº”è¯¥åœ¨å™ªå£°ä¸‹æ›´ç¨³å®š**
- å³ä½¿å®ƒä»¬çš„importanceç›¸åŒ

**å®éªŒè®¾è®¡ï¼š**

**æ­¥éª¤1ï¼šé€‰æ‹©ä¸¤ç»„æ—¶é—´æ­¥**
```python
categories = categorize_timesteps(attribution, trust)

# Group A: é«˜importance + é«˜trustï¼ˆå¯ä¿¡çš„é‡è¦ç‚¹ï¼‰
group_a = categories['reliable_important']

# Group B: é«˜importance + ä½trustï¼ˆä¸å¯ä¿¡çš„é‡è¦ç‚¹ï¼‰  
group_b = categories['unreliable_important']

# æ³¨æ„ï¼šä¸¤ç»„importanceéƒ½é«˜ï¼Œåªæœ‰trustä¸åŒ
```

**æ­¥éª¤2ï¼šåŠ å™ªå£°æµ‹è¯•**
```python
from enhanced_consistency_trust import TrustStabilityExperiment

exp = TrustStabilityExperiment(model, device='cuda')

# å¯¹Group AåŠ å™ªå£°
stability_a = exp.stability_under_noise(
    x, target_class, group_a,
    noise_levels=[0.1, 0.2, 0.3]
)

# å¯¹Group BåŠ å™ªå£°
stability_b = exp.stability_under_noise(
    x, target_class, group_b,
    noise_levels=[0.1, 0.2, 0.3]
)
```

**æ­¥éª¤3ï¼šå¯¹æ¯”ç»“æœ**
```python
print(f"Group A (é«˜trust):")
print(f"  é¢„æµ‹ç¨³å®šæ€§: {stability_a['prediction_stability']:.2%}")
print(f"  ç½®ä¿¡åº¦ä¸‹é™: {stability_a['confidence_drop']:.4f}")

print(f"Group B (ä½trust):")
print(f"  é¢„æµ‹ç¨³å®šæ€§: {stability_b['prediction_stability']:.2%}")
print(f"  ç½®ä¿¡åº¦ä¸‹é™: {stability_b['confidence_drop']:.4f}")
```

**é¢„æœŸç»“æœï¼š**
- Group A (é«˜trust): é¢„æµ‹ç¨³å®šæ€§é«˜ï¼ˆå¦‚ 80-90%ï¼‰ï¼Œç½®ä¿¡åº¦ä¸‹é™å°
- Group B (ä½trust): é¢„æµ‹ç¨³å®šæ€§ä½ï¼ˆå¦‚ 40-60%ï¼‰ï¼Œç½®ä¿¡åº¦ä¸‹é™å¤§

**ç»“è®ºï¼š**
> "This demonstrates that trust score captures explanation **reliability** 
> rather than attribution **importance**. High-trust timesteps remain stable 
> under perturbation, while low-trust timesteps (even with high importance) 
> are sensitive to noise, suggesting their attributions are unreliable."

**è®ºæ–‡ä¸­çš„ä½ç½®ï¼š**
- Section 4.4 Trust-aware Explanation Evaluation
- å¯ä»¥åšæˆä¸€ä¸ªå›¾ï¼šä¸¤ç»„çš„stabilityå¯¹æ¯”æŸ±çŠ¶å›¾

**æ–‡ä»¶ï¼š** `enhanced_consistency_trust.py` (TrustStabilityExperimentç±»)

---

## ğŸ“‚ æ–°å¢æ–‡ä»¶æ±‡æ€»

```
improved_files/
â”œâ”€â”€ enhanced_model_loader.py          # Transformerç­‰æ–°æ¨¡å‹
â”œâ”€â”€ separate_train_eval.py           # è®­ç»ƒè¯„ä¼°åˆ†ç¦»
â”œâ”€â”€ enhanced_consistency_trust.py    # æ”¹è¿›çš„ä¸€è‡´æ€§å’ŒTrust
â”œâ”€â”€ complete_improved_pipeline.py    # å®Œæ•´æ”¹è¿›æµç¨‹
â””â”€â”€ run_all_improvements.py          # ä¸€é”®è¿è¡Œæ‰€æœ‰æ”¹è¿›
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

**è¿è¡Œæ”¹è¿›ç‰ˆå®éªŒï¼š**
```bash
python run_all_improvements.py
```

**åˆ†æ­¥è¿è¡Œï¼š**
```python
# 1. è®­ç»ƒï¼ˆç‹¬ç«‹ï¼‰
from separate_train_eval import ModelTrainer
trainer = ModelTrainer(model)
trainer.train(train_loader, test_loader, epochs=50, model_name='my_model')

# 2. è¯„ä¼°ï¼ˆç‹¬ç«‹ï¼Œå¯ä»¥åœ¨ä¸åŒæ—¶é—´/åœ°ç‚¹ï¼‰
from separate_train_eval import ExplanationEvaluator
evaluator = ExplanationEvaluator('trained_models/my_model_best.pth')
evaluator.load_model(model)
results = evaluator.run_explanation_analysis(test_loader, pipeline)

# 3. è¿è¡Œæ”¹è¿›ç‰ˆå®éªŒ
from complete_improved_pipeline import CompleteExperimentPipeline
exp = CompleteExperimentPipeline(model)
exp.run_all_experiments(x, y, sample_name='sample_1')
```

## ğŸ“Š è®ºæ–‡å†™ä½œå»ºè®®

**Method Section éœ€è¦å¼ºè°ƒï¼š**
1. **Time-step-level consistency** - ä¸æ˜¯global view-level
2. **Trust â‰  Importance** - Truståæ˜ reliabilityï¼Œä¸æ˜¯saliency
3. **StabilityéªŒè¯** - é€šè¿‡å™ªå£°å®éªŒè¯æ˜trustçš„æ„ä¹‰

**Experiments Section ç»“æ„ï¼š**
```
4.2 Explanation Uncertainty Analysis
    â†’ å±•ç¤ºuncertaintyæœ‰æ„ä¹‰ï¼ˆå®éªŒ1ï¼‰

4.3 Cross-view Consistency Evaluation  
    â†’ å±•ç¤ºtime-step-levelä¸€è‡´æ€§ï¼ˆå®éªŒ2ï¼‰
    â†’ å¼ºè°ƒï¼šä¸æ˜¯global similarity

4.4 Trust-aware Explanation Evaluation
    â†’ åŒºåˆ†importance vs reliabilityï¼ˆå®éªŒ3ï¼‰
    â†’ ç¨³å®šæ€§éªŒè¯å®éªŒï¼ˆå®éªŒ4ï¼‰
    â†’ Deletionå®éªŒå¯¹æ¯”

4.5 Ablation Study
    â†’ éªŒè¯å„ç»„ä»¶å¿…è¦æ€§
```

**å…³é”®æœ¯è¯­ä½¿ç”¨ï¼š**
- **Attribution importance / magnitude** - æè¿°å½’å› å¤§å°
- **Explanation reliability / trustworthiness** - æè¿°å¯ä¿¡åº¦  
- **Time-step-level consistency** - å¼ºè°ƒä¸æ˜¯global
- **Stability under perturbation** - æè¿°trustçš„ç‰¹æ€§

## âœ… æ£€æŸ¥æ¸…å•

ä½¿ç”¨æ”¹è¿›ç‰ˆå‰ç¡®ä¿ï¼š
- [ ] å·²ä¿®å¤çš„ `deletion_experiment.py` å·²æ›¿æ¢åŸæ–‡ä»¶
- [ ] å¦‚æœç”¨Transformerï¼Œå¯¼å…¥ `enhanced_model_loader.py`
- [ ] å¦‚æœåˆ†ç¦»è®­ç»ƒè¯„ä¼°ï¼Œä½¿ç”¨ `separate_train_eval.py`
- [ ] å¦‚æœéœ€è¦time-step-levelï¼Œä½¿ç”¨ `enhanced_consistency_trust.py`
- [ ] è¿è¡Œå®Œæ•´æµç¨‹ï¼Œä½¿ç”¨ `run_all_improvements.py`

## ğŸ“ ä¸‹ä¸€æ­¥

1. åœ¨ä½ çš„çœŸå®æ•°æ®ä¸Šè¿è¡Œ `run_all_improvements.py`
2. æ”¶é›†å®éªŒç»“æœï¼ˆ4ä¸ªå®éªŒï¼‰
3. æ ¹æ®ç»“æœæ’°å†™è®ºæ–‡å®éªŒéƒ¨åˆ†
4. ä½¿ç”¨ç”Ÿæˆçš„å›¾è¡¨ï¼ˆè‡ªåŠ¨ä¿å­˜ï¼‰

ç¥å®éªŒé¡ºåˆ©ï¼ğŸš€
